# RunPod Serverless Dockerfile for NanoChat Inference
FROM pytorch/pytorch:2.4.0-cuda12.1-cudnn9-runtime

WORKDIR /workspace

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy uv from official image
COPY --from=ghcr.io/astral-sh/uv:0.6.10 /uv /bin/uv

# Install Rust for rustbpe tokenizer (minimal install)
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --profile minimal
ENV PATH="/root/.cargo/bin:${PATH}"

# Copy dependency files
COPY pyproject.toml uv.lock ./
COPY rustbpe ./rustbpe

# Install Python dependencies with uv (includes building rustbpe)
RUN uv pip install --system -e . && \
    uv pip install --system runpod && \
    rm -rf /root/.cargo/registry /root/.cargo/git && \
    rm -rf rustbpe/target/debug rustbpe/target/release/build rustbpe/target/release/deps rustbpe/target/release/incremental

# Copy source code
COPY src ./src

# Copy checkpoint (must exist at build time)
# Before building, copy your trained model to ./checkpoint/
# Example: cp -r data/sft_checkpoints/d4/* ./checkpoint/
COPY checkpoint /workspace/checkpoint

# Copy serverless handler
COPY serverless/handler.py ./

# Set Python path
ENV PYTHONPATH="/workspace/src:${PYTHONPATH}"

CMD ["python", "-u", "handler.py"]

